{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) Example with Ollama in Google Colab\n",
    "\n",
    "This notebook demonstrates how to set up a simple RAG example using Ollama's LLaVA model and LangChain. We will:\n",
    "- Install necessary libraries\n",
    "- Set up and run Ollama in the background\n",
    "- Download a sample PDF document\n",
    "- Embed document chunks using a vector database (ChromaDB)\n",
    "- Use Ollama's LLaVA model to answer queries based on document context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Ollama and start the service\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "!ollama serve &>/dev/null&  # Start Ollama in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Download LLaVA model (13b parameters, 1.6 version)\n",
    "!ollama pull llava:13b-v1.6  # Start Ollama in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Install additional Python packages for LangChain and PDF processing\n",
    "!pip install langchain_community pypdf requests langchain fastembed chromadb tiktoken sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Import Libraries and Define Configurations\n",
    "Import the necessary libraries and define configurations for ChromaDB and the document download path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "import requests\n",
    "\n",
    "# Set up configuration variables\n",
    "CHROMA_DATA_PATH = \"vdb_data/\"  # Path to store vector database data\n",
    "DOC_PATH = \"https://raw.githubusercontent.com/skillrepos/genai-dd/main/samples/data.pdf\"  # URL for sample PDF\n",
    "DOC_FILENAME = \"data.pdf\"  # Filename to save downloaded PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Download the Sample PDF Document\n",
    "We'll download the sample PDF document from the specified URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the PDF\n",
    "response = requests.get(DOC_PATH, allow_redirects=True)\n",
    "if response.status_code == 200:\n",
    "    with open(DOC_FILENAME, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"PDF downloaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to download PDF. Status code:\", response.status_code)\n",
    "    sys.exit(1)  # Exit if download failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Initialize the Ollama Model\n",
    "Make sure the Ollama server is running in the background. Load the LLaVA model with desired settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Ollama server if not already running\n",
    "!ollama serve &>/dev/null&  # Run in the background\n",
    "\n",
    "# Initialize the LLaVA model from Ollama\n",
    "llm = Ollama(model=\"llava:13b-v1.6\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Load and Split PDF Document into Chunks\n",
    "We will load the document into memory and split it into smaller chunks for more efficient retrieval during querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split the document into smaller chunks\n",
    "loader = PyPDFLoader(DOC_PATH)\n",
    "pages = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"Document split into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Embed Chunks and Create Vector Database\n",
    "Embed the chunks into vectors using FastEmbed and store them in ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed document chunks and load them into ChromaDB\n",
    "embeddings = FastEmbedEmbeddings()\n",
    "db_chroma = Chroma.from_documents(chunks, embeddings, persist_directory=CHROMA_DATA_PATH)\n",
    "\n",
    "print(\"Vector database created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Define the Query Template and Run Queries\n",
    "Define a prompt template and set up a loop to allow interactive queries. Type `exit` to stop the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question: {question} using whatever resources you have.\n",
    "Include any related information from {context} as part of your answer.\n",
    "Provide a detailed answer.\n",
    "Donâ€™t justify your answers.\n",
    "\"\"\"\n",
    "\n",
    "# Interactive query loop\n",
    "while True:\n",
    "    query = input(\"\\nQuery: \")\n",
    "    if query.lower() == \"exit\":\n",
    "        print(\"Exiting...\")\n",
    "        break\n",
    "    if not query.strip():\n",
    "        continue\n",
    "    \n",
    "    # Retrieve the top 5 most relevant chunks\n",
    "    docs_chroma = db_chroma.similarity_search_with_score(query, k=5)\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc, _score in docs_chroma])\n",
    "    \n",
    "    # Prepare the prompt\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query)\n",
    "\n",
    "    # Generate and display the response\n",
    "    response_text = llm.invoke(prompt)\n",
    "    print(response_text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Ollama_RAG_Example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
